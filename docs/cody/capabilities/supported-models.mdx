# Supported Large Language Models

## Chat and Commands

Cody supports a variety of cutting edge large language models for use in Chat and Commands, allowing you to select the best model for your use case.  Free users are defaulted to the Claude 3 Sonnet model from Anthropic, while Pro users have access to select any supported model.

| **Provider** |                                                                   **Model**                                                                   |    **Free**    |    **Pro**     | **Enterprise** |
| :----------- | :-------------------------------------------------------------------------------------------------------------------------------------------- | :------------- | :------------- | :------------- |
| OpenAI       | [gpt-3.5 turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)                                                                        | -              | ✅              | ✅              |
| OpenAI       | [gpt-4](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo#:~:text=to%20Apr%202023-,gpt%2D4,-Currently%20points%20to)              | -              | -              | ✅              |
| OpenAI       | [gpt-4 turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo#:~:text=TRAINING%20DATA-,gpt%2D4%2D0125%2Dpreview,-New%20GPT%2D4) | -              | ✅              | ✅              |
| OpenAI       | [gpt-4o](https://platform.openai.com/docs/models/gpt-4o) | -              | ✅              | ✅              |
| Anthropic    | [claude-3 Haiku](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                     | -              | ✅              | ✅              |
| Anthropic    | [claude-3 Sonnet](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                    | ✅              | ✅              | ✅              |
| Anthropic    | [claude-3 Opus](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                      | -              | ✅              | ✅              |
| Mistral      | [mixtral 8x7b](https://mistral.ai/technology/#models:~:text=of%20use%20cases.-,Mixtral%208x7B,-Currently%20the%20best)                        | -              | ✅              | -              |
| Mistral      | [mixtral 8x22b](https://mistral.ai/technology/#models:~:text=of%20use%20cases.-,Mixtral%208x7B,-Currently%20the%20best)                       | -              | ✅              | -              |
| Ollama      | [variety](https://ollama.com/)                                                                                                                | experimental | experimental | -              |
| Google Gemini      | [1.5 Pro](https://deepmind.google/technologies/gemini/pro/)                                                                                                                | - | ✅ | ✅              |
| Google Gemini       | [1.5 Flash](https://deepmind.google/technologies/gemini/flash/)                                                                                                                | - | ✅ | ✅              |
|              |                                                                                                                                               |                |                |                |

<Callout type="note">To use Claude 3 (Haiku, Sonnet and Opus), GPT-4o and Google Gemini models with Cody Enterprise, make sure you've upgraded your Sourcegraph instance to the latest version.</Callout>

## Autocomplete

Cody uses a set of models for autocomplete which are suited for the low latency use case.

| **Provider** |                                         **Model**                                         |    **Free**    |    **Pro**     | **Enterprise** |
| :----------- | :---------------------------------------------------------------------------------------- | :------------- | :------------- | :------------- |
| Fireworks.ai | [StarCoder](https://arxiv.org/abs/2305.06161)                                             | ✅              | ✅              | ✅              |
| Anthropic    | [claude Instant](https://docs.anthropic.com/claude/docs/models-overview#model-comparison) | -              | -              | ✅              |
| Ollama*      | [variety](https://ollama.com/)                                                            | *experimental* | *experimental* | -              |
|              |                                                                                           |                |                |                |

<Callout type="note">[See here for Ollama setup instructions](https://sourcegraph.com/docs/cody/clients/install-vscode#supported-local-ollama-models-with-cody)</Callout>

## Input and output token limits

For all models, Cody allows up to **4,000 tokens of output**, which is approximately **500-600** lines of code.

For Claude 3 Sonnet or Opus models, Cody tracks two separate token limits:
* @-mention context is limited to 30,000 tokens (~4,000 lines of code) and can be specified using the @-filename syntax. This context is explicitly defined by the user and is used to provide specific information to Cody.
* Conversation context is limited to 15,000 tokens and includes user questions, system responses, and automatically retrieved context items. Apart from user questions, this context is generated automatically by Cody.

All other models are currently capped at **7,000 tokens** of shared context between `@-mention` context and chat history.

Here's a detailed breakdown of the token limits by model:



<Tabs>
  <Tab title="Free & Pro">
|      **Model**      | **Conversation Context** | **@-mention Context** | **Output** |
| ------------------- | ------------------------ | --------------------- | ---------- |
| gpt-3.5-turbo       | 3,500                    | shared                | 1,000      |
| gpt-4               | 7,000                    | shared                | 4,000      |
| gpt-4-turbo         | 7,000                    | shared                | 4,000      |
| claude instant      | 7,000                    | shared                | 4,000      |
| claude-2.0          | 7,000                    | shared                | 4,000      |
| claude-2.1          | 7,000                    | shared                | 4,000      |
| claude-3 Haiku      | 7,000                    | shared                | 4,000      |
| **claude-3 Sonnet** | **15,000**               | **30,000**            | **4,000**  |
| **claude-3 Opus**   | **15,000**               | **30,000**            | **4,000**  |
| mixtral 8x7b        | 7,000                    | shared                | 4,000      |
  </Tab>
  <Tab title="Free & Pro">
|      **Model**      | **Conversation Context** | **@-mention Context** | **Output** |
| ------------------- | ------------------------ | --------------------- | ---------- |
| gpt-3.5-turbo       | 7,000                    | shared                | 4,000      |
| gpt-4               | 7,000                    | shared                | 4,000      |
| gpt-4-turbo         | 7,000                    | shared                | 4,000      |
| claude instant      | 7,000                    | shared                | 4,000      |
| claude-2.0          | 7,000                    | shared                | 4,000      |
| claude-2.1          | 7,000                    | shared                | 4,000      |
| claude-3 Haiku      | 7,000                    | shared                | 4,000      |
| **claude-3 Sonnet** | **15,000**               | **30,000**            | **4,000**  |
| **claude-3 Opus**   | **15,000**               | **30,000**            | **4,000**  |
| mixtral 8x7b        | 7,000                    | shared                | 4,000      |
   </Tab>
</Tabs>

For more information on how Cody builds context, see our [documentation here](/cody/core-concepts/context).
