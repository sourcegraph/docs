# Supported LLMs

## Chat and Commands

Cody supports a variety of cutting-edge large language models for use in Chat and Commands, allowing you to select the best model for your use case.

<Callout type="note">Newer versions of Sourcegraph Enterprise, starting from v5.6, it will be even easier to add support for new models and providers, see [Model Configuration](/cody/clients/model-configuration) for more information.</Callout>

| **Provider**  |                                                                   **Model**                                                                   |   **Free**   |   **Pro**    | **Enterprise** |     |     |     |     |
| :------------ | :-------------------------------------------------------------------------------------------------------------------------------------------- | :----------- | :----------- | :------------- | --- | --- | --- | --- |
| OpenAI        | [gpt-3.5 turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)                                                                        | ✅            | ✅            | ✅              |     |     |     |     |
| OpenAI        | [gpt-4](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo#:~:text=to%20Apr%202023-,gpt%2D4,-Currently%20points%20to)              | -            | -            | ✅              |     |     |     |     |
| OpenAI        | [gpt-4 turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo#:~:text=TRAINING%20DATA-,gpt%2D4%2D0125%2Dpreview,-New%20GPT%2D4) | -            | ✅            | ✅              |     |     |     |     |
| OpenAI        | [gpt-4o](https://platform.openai.com/docs/models/gpt-4o)                                                                                      | -            | ✅            | ✅              |     |     |     |     |
| Anthropic     | [claude-3 Haiku](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                     | ✅            | ✅            | ✅              |     |     |     |     |
| Anthropic     | [claude-3 Sonnet](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                    | ✅            | ✅            | ✅              |     |     |     |     |
| Anthropic     | [claude-3.5 Sonnet](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                  | ✅            | ✅            | ✅              |     |     |     |     |
| Anthropic     | [claude-3 Opus](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                      | -            | ✅            | ✅              |     |     |     |     |
| Mistral       | [mixtral 8x7b](https://mistral.ai/technology/#models:~:text=of%20use%20cases.-,Mixtral%208x7B,-Currently%20the%20best)                        | ✅            | ✅            | -              |     |     |     |     |
| Mistral       | [mixtral 8x22b](https://mistral.ai/technology/#models:~:text=of%20use%20cases.-,Mixtral%208x7B,-Currently%20the%20best)                       | ✅            | ✅            | -              |     |     |     |     |
| Ollama        | [variety](https://ollama.com/)                                                                                                                | experimental | experimental | -              |     |     |     |     |
| Google Gemini | [1.5 Pro](https://deepmind.google/technologies/gemini/pro/)                                                                                   | ✅            | ✅            | ✅ (Beta)       |     |     |     |     |
| Google Gemini | [1.5 Flash](https://deepmind.google/technologies/gemini/flash/)                                                                               | ✅            | ✅            | ✅ (Beta)       |     |     |     |     |
|               |                                                                                                                                               |              |              |                |     |     |     |     |

<Callout type="note">To use Claude 3 (Opus and Sonnets) models with Cody Enterprise, make sure you've upgraded your Sourcegraph instance to the latest version.</Callout>

## Autocomplete

Cody uses a set of models for autocomplete which are suited for the low latency use case.

|     **Provider**      |                                         **Model**                                         | **Free** | **Pro** | **Enterprise** |     |     |     |     |
| :-------------------- | :---------------------------------------------------------------------------------------- | :------- | :------ | :------------- | --- | --- | --- | --- |
| Fireworks.ai          | [DeepSeek-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2)                             | ✅        | ✅       | ✅              |     |     |     |     |
| Fireworks.ai          | [StarCoder](https://arxiv.org/abs/2305.06161)                                             | -        | -       | ✅              |     |     |     |     |
| Anthropic             | [claude Instant](https://docs.anthropic.com/claude/docs/models-overview#model-comparison) | -        | -       | ✅              |     |     |     |     |
| Google Gemini (Beta)  | [1.5 Flash](https://deepmind.google/technologies/gemini/flash/)                           | -        | -       | ✅              |     |     |     |     |
| Ollama (Experimental) | [variety](https://ollama.com/)                                                            | ✅        | ✅       | -              |     |     |     |     |
|                       |                                                                                           |          |         |                |     |     |     |     |

<Callout type="note">The default autocomplete model for Cody Free and Pro user is DeepSeek-V2. Enterprise users get StarCoder as the default model.</Callout>

Read here for [Ollama setup instructions](https://sourcegraph.com/docs/cody/clients/install-vscode#supported-local-ollama-models-with-cody). For information on context token limits, see our [documentation here](/cody/core-concepts/token-limits).
