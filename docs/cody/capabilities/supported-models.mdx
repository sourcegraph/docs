# Supported Large Language Models

## Chat and Commands

Cody supports a variety of cutting edge large language models for use in Chat and Commands, allowing you to select the best model for your use case.  Free users are defaulted to the Claude 3 Sonnet model from Anthropic, while Pro users have access to select any supported model.

| **Provider** |                                                                   **Model**                                                                   |    **Free**    |    **Pro**     | **Enterprise** |
| :----------- | :-------------------------------------------------------------------------------------------------------------------------------------------- | :------------- | :------------- | :------------- |
| OpenAI       | [gpt-3.5 turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)                                                                        | -              | ✅              | ✅              |
| OpenAI       | [gpt-4](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo#:~:text=to%20Apr%202023-,gpt%2D4,-Currently%20points%20to)              | -              | -              | ✅              |
| OpenAI       | [gpt-4 turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo#:~:text=TRAINING%20DATA-,gpt%2D4%2D0125%2Dpreview,-New%20GPT%2D4) | -              | ✅              | ✅              |
| OpenAI       | [gpt-4o](https://platform.openai.com/docs/models/gpt-4o) | -              | ✅              | ✅              |
| Anthropic    | [claude-3 Haiku](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                     | -              | ✅              | ✅              |
| Anthropic    | [claude-3 Sonnet](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                    | ✅              | ✅              | ✅              |
| Anthropic    | [claude-3 Opus](https://docs.anthropic.com/claude/docs/models-overview#model-comparison)                                                      | -              | ✅              | ✅              |
| Mistral      | [mixtral 8x7b](https://mistral.ai/technology/#models:~:text=of%20use%20cases.-,Mixtral%208x7B,-Currently%20the%20best)                        | -              | ✅              | -              |
| Mistral      | [mixtral 8x22b](https://mistral.ai/technology/#models:~:text=of%20use%20cases.-,Mixtral%208x7B,-Currently%20the%20best)                       | -              | ✅              | -              |
| Ollama      | [variety](https://ollama.com/)                                                                                                                | experimental | experimental | -              |
| Google Gemini      | [1.5 Pro](https://deepmind.google/technologies/gemini/pro/)                                                                                                                | - | ✅ | ✅              |
| Google Gemini       | [1.5 Flash](https://deepmind.google/technologies/gemini/flash/)                                                                                                                | - | ✅ | ✅              |
|              |                                                                                                                                               |                |                |                |                                                                                                                                           |                |                |                |

<Callout type="note">To use Claude 3 (Opus and Sonnets) models with Cody Enterprise, make sure you've upgraded your Sourcegraph instance to the latest version.</Callout>

## Autocomplete

Cody uses a set of models for autocomplete which are suited for the low latency use case.

| **Provider** |                                         **Model**                                         |    **Free**    |    **Pro**     | **Enterprise** |
| :----------- | :---------------------------------------------------------------------------------------- | :------------- | :------------- | :------------- |
| Fireworks.ai | [StarCoder](https://arxiv.org/abs/2305.06161)                                             | ✅              | ✅              | ✅              |
| Anthropic    | [claude Instant](https://docs.anthropic.com/claude/docs/models-overview#model-comparison) | -              | -              | ✅              |
| Ollama*      | [variety](https://ollama.com/)                                                            | *experimental* | *experimental* | -              |
|              |                                                                                           |                |                |                |

<Callout type="note">[See here for Ollama setup instructions](https://sourcegraph.com/docs/cody/clients/install-vscode#supported-local-ollama-models-with-cody)</Callout>

For information on context token limits, see our [documentation here](/cody/core-concepts/token-limits).
