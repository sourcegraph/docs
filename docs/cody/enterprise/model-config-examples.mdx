# Model Configuration Examples

<p className="subtitle">
	This section includes examples about how to configure Cody to use
	Sourcegraph-provided models with `modelConfiguration`. These examples will
	use the following:
</p>

-   [Minimal configuration](/cody/enterprise/model-configuration#configure-sourcegraph-provided-models)
-   [Using model filters](/cody/enterprise/model-configuration#model-filters)
-   [Change default models](/cody/enterprise/model-configuration#default-models)

## Sourcegraph-provided models and BYOK (Bring Your Own Key)

By default, Sourcegraph is fully aware of several models from the following providers:

-   "anthropic"
-   "google"
-   "fireworks"
-   "mistral"
-   "openai"

### Override configuration of a model provider

Instead of Sourcegraph using its own servers to make LLM requests, it is possible to bring your own API keys for a given model provider. For example, if you wish for all Anthropic API requests to go directly to your own Anthropic account and use your own API keys instead of going via Sourcegraph's servers, you could override the `anthropic` provider's configuration:

```json
{
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": {},
  "providerOverrides": [
    {
     "id": "anthropic",
    "displayName": "Anthropic BYOK",
    "serverSideConfig": {
      "type": "anthropic",
      "accessToken": "token",
       "endpoint": "https://api.anthropic.com/v1/messages"
      }
    }
  ],
  "defaultModels": {
    "chat": "anthropic::2024-10-22::claude-3.5-sonnet",
    "fastChat": "anthropic::2023-06-01::claude-3-haiku",
    "autocomplete": "fireworks::v1::deepseek-coder-v2-lite-base"
  }
}
```

In the configuration above:

-   Enable Sourcegraph-provided models and do not set any overrides (note that `"modelConfiguration.modelOverrides"` is not specified)
-   Route requests for Anthropic models directly to the Anthropic API (via the provider override specified for "anthropic")
-   Route requests for other models (such as the Fireworks model for "autocomplete") through Cody Gateway

### Partially override provider config in the namespace

If you want to override the provider config for some models in the namespace and use the Sourcegraph-configured provider config for the rest, you can route requests directly to the LLM provider (bypassing the Cody Gateway) for some models while using the Sourcegraph-configured provider config for the rest.

Example configuration

```json
{
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": {},
  "providerOverrides": [
    {
      "id": "anthropic-byok",
      "displayName": "Anthropic BYOK",
      "serverSideConfig": {
          "type": "anthropic",
          "accessToken": "token",
          "endpoint": "https://api.anthropic.com/v1/messages"
        }
    }
  ],
  "modelOverrides": [
    {
      "modelRef": "anthropic-byok::2023-06-01::claude-3.5-sonnet",
      "displayName": "Claude 3.5 Sonnet",
      "modelName": "claude-3-5-sonnet-latest",
      "capabilities": ["edit", "chat"],
      "category": "accuracy",
      "status": "stable",
      "contextWindow": {
        "maxInputTokens": 45000,
        "maxOutputTokens": 4000
      }
    },
  ],
  "defaultModels": {
    "chat": "anthropic-byok::2023-06-01::claude-3.5-sonnet",
    "fastChat": "anthropic::2023-06-01::claude-3-haiku",
    "autocomplete": "fireworks::v1::deepseek-coder-v2-lite-base"
  }
}
```

In the configuration above, we:

-   Enable Sourcegraph-supplied models (the `sourcegraph` field is not empty or `null`)
-   Define a new provider with the ID `"anthropic-byok"` and configure it to use the Anthropic API
-   Since this provider is unknown to Sourcegraph, no Sourcegraph-supplied models are available. Therefore, we add a custom model in the `"modelOverrides"` section
-   Use the custom model configured in the previous step (`"anthropic-byok::2024-10-22::claude-3.5-sonnet"`) for `"chat"`. Requests are sent directly to the Anthropic API as set in the provider override
-   For `"fastChat"` and `"autocomplete"`, we use Sourcegraph-provided models via Cody Gateway

## Config examples for various LLM providers

Below are configuration examples for setting up various LLM providers using BYOK. These examples are applicable whether or not you are using Sourcegraph-supported models.

-   In this section, all configuration examples have Sourcegraph-provided models disabled. Please refer to the previous section to use a combination of Sourcegraph-provided models and BYOK.
-   Ensure that at least one model is available for each Cody feature ("chat" and "autocomplete"), regardless of the provider and model overrides configured. To verify this, [view the configuration](/cody/enterprise/model-configuration#view-configuration) and confirm that appropriate models are listed in the `"defaultModels"` section.

<Accordion title="Anthropic">

```json
{
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": null,
  "providerOverrides": [
    {
     "id": "anthropic",
      "displayName": "Anthropic",
      "serverSideConfig": {
      "type": "anthropic",
      "accessToken": "token",
        "endpoint": "https://api.anthropic.com/v1/messages"
      }
    }
  ],
  "modelOverrides": [
    {
      "modelRef": "anthropic::2024-10-22::claude-3.5-sonnet",
      "displayName": "Claude 3.5 Sonnet",
      "modelName": "claude-3-5-sonnet-latest",
      "capabilities": ["chat"],
      "category": "accuracy",
      "status": "stable",
      "contextWindow": {
        "maxInputTokens": 45000,
        "maxOutputTokens": 4000
      }
    },
    {
      "modelRef": "anthropic::2023-06-01::claude-3-haiku",
      "displayName": "Claude 3 Haiku",
      "modelName": "claude-3-haiku-20240307",
      "capabilities": ["chat"],
      "category": "speed",
      "status": "stable",
      "contextWindow": {
          "maxInputTokens": 7000,
          "maxOutputTokens": 4000
      }
    },
    {
      "modelRef": "anthropic::2023-06-01::claude-3-haiku",
      "displayName": "Claude 3 Haiku",
      "modelName": "claude-3-haiku-20240307",
      "capabilities": ["edit", "chat"],
      "category": "speed",
      "status": "stable",
      "contextWindow": {
        "maxInputTokens": 7000,
        "maxOutputTokens": 4000
      }
    }
  ],
  "defaultModels": {
    "chat": "anthropic::2024-10-22::claude-3.5-sonnet",
    "fastChat": "anthropic::2023-06-01::claude-3-haiku",
    "autocomplete": "fireworks::v1::deepseek-coder-v2-lite-base"
  }
}
```

In the configuration above,

-   Set up a provider override for Anthropic, routing requests for this provider directly to the specified Anthropic endpoint (bypassing Cody Gateway)
-   Add three Anthropic models:
    -   Two models with chat capabilities (`"anthropic::2024-10-22::claude-3.5-sonnet"` and `"anthropic::2023-06-01::claude-3-haiku"`), providing options for chat users
    -   One model with autocomplete capability (`"fireworks::v1::deepseek-coder-v2-lite-base"`)
-   Set the configured models as default models for Cody features in the `"defaultModels"` field

</Accordion>

<Accordion title="Fireworks">
```json
"cody.enabled": true,
"modelConfiguration": {
 "sourcegraph": null,
 "providerOverrides": [
 {
 "id": "fireworks",
 "displayName": "Fireworks",
 "serverSideConfig": {
 "type": "fireworks",
 "accessToken": "token",
 "endpoint": "https://api.fireworks.ai/inference/v1/completions"
 }
 }
 ],
 "modelOverrides": [
 {
 "modelRef": "fireworks::v1::mixtral-8x22b-instruct",
 "displayName": "Mixtral 8x22B",
 "modelName": "accounts/fireworks/models/mixtral-8x22b-instruct",
 "capabilities": ["chat"],
 "category": "other",
 "status": "stable",
 "contextWindow": {
 "maxInputTokens": 7000,
 "maxOutputTokens": 4000
 }
 },
 {
 "modelRef": "fireworks::v1::starcoder-16b",
 "modelName": "accounts/fireworks/models/starcoder-16b",
 "displayName": "(Fireworks) Starcoder 16B",
 "contextWindow": {
 "maxInputTokens": 8192,
 "maxOutputTokens": 4096
 },
 "capabilities": ["autocomplete"],
 "category": "balanced",
 "status": "stable"
 }
 ],
 "defaultModels": {
 "chat": "fireworks::v1::mixtral-8x22b-instruct",
 "fastChat": "fireworks::v1::mixtral-8x22b-instruct",
 "autocomplete": "fireworks::v1::starcoder-16b"
 }
}
```

In the configuration above,

-   Set up a provider override for Fireworks, routing requests for this provider directly to the specified Fireworks endpoint (bypassing Cody Gateway)
-   Add two Fireworks models:
    -   `"fireworks::v1::mixtral-8x22b-instruct"` with "chat" capabiity - used for "chat" and "fastChat"
    -   `"fireworks::v1::starcoder-16b"` with "autocomplete" capability - used for "autocomplete"

</Accordion>

<Accordion title="OpenAI">

```json
"modelConfiguration": {
  "sourcegraph": null,
  "providerOverrides": [
    {
      "id": "openai",
      "displayName": "OpenAI",
      "serverSideConfig": {
        "type": "openai",
        "accessToken": "token",
        "endpoint": "https://api.openai.com"
      }
    }
  ],
  "modelOverrides": [
    {
      "modelRef": "openai::2024-02-01::gpt-4o",
      "displayName": "GPT-4o",
      "modelName": "gpt-4o",
      "capabilities": ["chat"],
      "category": "accuracy",
      "status": "stable",
      "contextWindow": {
          "maxInputTokens": 45000,
          "maxOutputTokens": 4000
      }
    },
    {
      "modelRef": "openai::unknown::gpt-3.5-turbo-instruct",
      "displayName": "GPT-3.5 Turbo Instruct",
      "modelName": "gpt-3.5-turbo-instruct",
      "capabilities": ["autocomplete"],
      "category": "speed",
      "status": "stable",
      "contextWindow": {
          "maxInputTokens": 7000,
          "maxOutputTokens": 4000
      }
    }
],
  "defaultModels": {
    "chat": "openai::2024-02-01::gpt-4o",
    "fastChat": "openai::2024-02-01::gpt-4o",
    "autocomplete": "openai::unknown::gpt-3.5-turbo-instruct"
  }
}
```

In the configuration above,

-   Set up a provider override for OpenAI, routing requests for this provider directly to the specified OpenAI endpoint (bypassing Cody Gateway)
-   Add two OpenAI models:
    -   `"openai::2024-02-01::gpt-4o"` with "chat" capabilities - used for "chat" and "fastChat"
    -   `"openai::unknown::gpt-3.5-turbo-instruct"` with "autocomplete" capability - used for "autocomplete"

</Accordion>

<Accordion title="Azure OpenAI">

```json
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": null,
  "providerOverrides": [
    {
        "id": "azure-openai",
        "displayName": "Azure OpenAI",
        "serverSideConfig": {
          "type": "azureOpenAI",
          "accessToken": "token",
          "endpoint": "https://acme-test.openai.azure.com/",
          "user": "",
          "useDeprecatedCompletionsAPI": true
        }
    }
  ],
  "modelOverrides": [
      {
        "modelRef": "azure-openai::unknown::gpt-4o",
        "displayName": "GPT-4o",
        "modelName": "gpt-4o",
        "capabilities": ["chat"],
        "category": "accuracy",
        "status": "stable",
        "contextWindow": {
            "maxInputTokens": 45000,
            "maxOutputTokens": 4000
        }
      },
      {
        "modelRef": "azure-openai::unknown::gpt-35-turbo-instruct-test",
        "displayName": "GPT-3.5 Turbo Instruct",
        "modelName": "gpt-35-turbo-instruct-test",
        "capabilities": ["autocomplete"],
        "category": "speed",
        "status": "stable",
        "contextWindow": {
            "maxInputTokens": 7000,
            "maxOutputTokens": 4000
        }
      }
  ],
  "defaultModels": {
    "chat": "azure-openai::unknown::gpt-4o",
    "fastChat": "azure-openai::unknown::gpt-4o",
    "autocomplete": "azure-openai::unknown::gpt-35-turbo-instruct-test"
  }
}
```

In the configuration above,
In the configuration above,

-   Set up a provider override for Azure OpenAI, routing requests for this provider directly to the specified Azure OpenAI endpoint (bypassing Cody Gateway).
    **Note:** For Azure OpenAI, ensure that the `modelName` matches the name defined in your Azure portal configuration for the model.
-   Add two OpenAI models:
    -   `"azure-openai::unknown::gpt-4o"` with "chat" capability - used for "chat" and "fastChat"
    -   `"azure-openai::unknown::gpt-35-turbo-instruct-test"` with "autocomplete" capability - used for "autocomplete"
-   Since `"azure-openai::unknown::gpt-35-turbo-instruct-test"` is not supported on the newer OpenAI `"v1/chat/completions"` endpoint, we set `"useDeprecatedCompletionsAPI"` to `true` to route requests to the legacy `"v1/completions"` endpoint. This setting is unnecessary if you are using a model supported on the `"v1/chat/completions"` endpoint.

</Accordion>

<Accordion title="Generic OpenAI-compatible">

```json
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": null,
  "providerOverrides": [
    {
      "id": "fireworks",
      "displayName": "Fireworks",
      "serverSideConfig": {
        "type": "openaicompatible",
        "endpoints": [
          {
              "url": "https://api.fireworks.ai/inference/v1",
              "accessToken": "token"
          }
        ]
      }
    },
    {
      "id": "huggingface-codellama",
      "displayName": "Hugging Face",
      "serverSideConfig": {
        "type": "openaicompatible",
        "endpoints": [
          {
            "url": "https://api-inference.huggingface.co/models/meta-llama/CodeLlama-7b-hf/v1/",
            "accessToken": "token"
          }
        ]
      }
    },
  ],
  "modelOverrides": [
    {
      "modelRef": "fireworks::v1::llama-v3p1-70b-instruct",
      "modelName": "llama-v3p1-70b-instruct",
      "displayName": "(Fireworks) Llama 3.1 70B Instruct",
      "contextWindow": {
        "maxInputTokens": 64000,
        "maxOutputTokens": 8192
      },
      "serverSideConfig": {
        "type": "openaicompatible",
        "apiModel": "accounts/fireworks/models/llama-v3p1-70b-instruct"
      },
      "clientSideConfig": {
        "openaicompatible": {}
      },
      "capabilities": ["chat"],
      "category": "balanced",
      "status": "stable"
    },
    {
      "modelRef": "huggingface-codellama::v1::CodeLlama-7b-hf",
      "modelName": "CodeLlama-7b-hf",
      "displayName": "(HuggingFace) CodeLlama-7b-hf",
      "contextWindow": {
        "maxInputTokens": 8192,
        "maxOutputTokens": 4096
      },
      "serverSideConfig": {
        "type": "openaicompatible",
        "apiModel": "meta-llama/CodeLlama-7b-hf"
      },
      "clientSideConfig": {
        "openaicompatible": {}
      },
      "capabilities": ["autocomplete", "chat"],
      "category": "balanced",
      "status": "stable"
    }
  ],
  "defaultModels": {
    "chat": "fireworks::v1::llama-v3p1-70b-instruct",
    "fastChat": "fireworks::v1::llama-v3p1-70b-instruct",
    "autocomplete": "huggingface-codellama::v1::CodeLlama-7b-hf"
  }
}
```

In the configuration above,

-   Configure two OpenAI-compatible providers: `"fireworks"` and `"huggingface-codellama"`
-   Add two OpenAI-compatible models: `"fireworks::v1::llama-v3p1-70b-instruct"` and `"huggingface-codellama::v1::CodeLlama-7b-hf"`. Additionally:
    -   Set `clientSideConfig.openaicompatible` to `{}` to indicate to Cody clients that these models are OpenAI-compatible, ensuring the appropriate code paths are utilized
    -   Designate these models as the default choices for chat and autocomplete, respectively

</Accordion>

<Accordion title="Google Vertex (Anthropic)">

```json
"modelConfiguration": {
  "sourcegraph": null,
  "providerOverrides": [
      {
          "id": "google",
          "displayName": "Google Anthropic",
          "serverSideConfig": {
            "type": "google",
            "accessToken": "token",
            "endpoint": "https://us-east5-aiplatform.googleapis.com/v1/projects/project-name/locations/us-east5/publishers/anthropic/models"
          }
      }
  ],
  "modelOverrides": [
      {
          "modelRef": "google::unknown::claude-3-5-sonnet",
          "displayName": "Claude 3.5 Sonnet (via Google/Vertex)",
          "modelName": "claude-3-5-sonnet@20240620",
          "contextWindow": {
            "maxInputTokens": 45000,
            "maxOutputTokens": 4000
          },
          "capabilities": ["chat"],
          "category": "accuracy",
          "status": "stable"
      },
      {
          "modelRef": "google::unknown::claude-3-haiku",
          "displayName": "Claude 3 Haiku",
          "modelName": "claude-3-haiku@20240307",
          "capabilities": ["autocomplete", "chat"],
          "category": "speed",
          "status": "stable",
          "contextWindow": {
            "maxInputTokens": 7000,
            "maxOutputTokens": 4000
          }
      },
  ],
  "defaultModels": {
    "chat": "google::unknown::claude-3-5-sonnet",
    "fastChat": "google::unknown::claude-3-5-sonnet",
    "autocomplete": "google::unknown::claude-3-haiku"
  }
}
```

In the configuration above,

-   Set up a provider override for Google Anthropic, routing requests for this provider directly to the specified endpoint (bypassing Cody Gateway)
-   Add two Anthropic models:
    -   `"google::unknown::claude-3-5-sonnet"` with "chat" capabiity - used for "chat" and "fastChat"
    -   `"google::unknown::claude-3-haiku"` with "autocomplete" capability - used for "autocomplete"

</Accordion>

<Accordion title="Google Vertex (Gemini)">

```json
"modelConfiguration": {
  "sourcegraph": null,
  "providerOverrides": [
      {
          "id": "google",
          "displayName": "Google Gemini",
          "serverSideConfig": {
            "type": "google",
            "accessToken": "token",
            "endpoint": "https://generativelanguage.googleapis.com/v1beta/models"
          }
      }
  ],
  "modelOverrides": [
    {
      "modelRef": "google::v1::gemini-1.5-pro",
      "displayName": "Gemini 1.5 Pro",
      "modelName": "gemini-1.5-pro",
      "capabilities": ["chat", "autocomplete"],
      "category": "balanced",
      "status": "stable",
      "contextWindow": {
        "maxInputTokens": 45000,
        "maxOutputTokens": 4000
      }
    }
  ],
  "defaultModels": {
    "chat": "google::v1::gemini-1.5-pro",
    "fastChat": "google::v1::gemini-1.5-pro",
    "autocomplete": "google::v1::gemini-1.5-pro"
  }
}
```

In the configuration above,

-   Set up a provider override for Google Gemini, routing requests for this provider directly to the specified endpoint (bypassing Cody Gateway)
-   Add the `"google::v1::gemini-1.5-pro"` model, which is used for all Cody features. We do not add other models for simplicity, as adding multiple models is already covered in the examples above

</Accordion>

<Accordion title="AWS Bedrock">

```json
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": null,
  "providerOverrides": [
      {
          "id": "aws-bedrock",
          "displayName": "AWS Bedrock",
          "serverSideConfig": {
              "type": "awsBedrock",
              "accessToken": "token",
              "endpoint": "us-west-2",
              "region": "us-west-2"
          }
      }
  ],
  "modelOverrides": [
      {
          "modelRef": "aws-bedrock::2023-06-01::claude-3-opus",
          "displayName": "Claude 3 Opus (AWS Bedrock)",
          "modelName": "anthropic.claude-3-opus-20240229-v1:0",
          "capabilities": ["chat", "autocomplete"],
          "category": "other",
          "status": "stable",
          "contextWindow": {
              "maxInputTokens": 45000,
              "maxOutputTokens": 4000
          }
      }
  ],
  "defaultModels": {
      "chat": "aws-bedrock::2023-06-01::claude-3-opus",
      "fastChat": "aws-bedrock::2023-06-01::claude-3-opus",
      "autocomplete": "aws-bedrock::2023-06-01::claude-3-opus",
  }
}
```

In the configuration described above,

-   Set up a provider override for AWS Bedrock, routing requests for this provider directly to the specified endpoint, bypassing Cody Gateway
-   Add the `"aws-bedrock::2023-06-01::claude-3-opus"` model, which is used for all Cody features. We do not add other models for simplicity, as adding multiple models is already covered in the examples above

Provider override `serverSideConfig` fields:

| **Field**     | **Description**                                                                                                                                                                                                                                                                                                 |
| ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `type`        | Must be `"awsBedrock"`.                                                                                                                                                                                                                                                                                         |
| `accessToken` | Leave empty to rely on instance role bindings or other AWS configurations in the frontend service. Use `<ACCESS_KEY_ID>:<SECRET_ACCESS_KEY>` for direct credential configuration, or `<ACCESS_KEY_ID>:<SECRET_ACCESS_KEY>:<SESSION_TOKEN>` if a session token is also required.                                 |
| `endpoint`    | For pay-as-you-go, set it to an AWS region code (e.g., `us-west-2`) when using a public Amazon Bedrock endpoint. For provisioned throughput, set it to the provisioned VPC endpoint for the bedrock-runtime API (e.g., `https://vpce-0a10b2345cd67e89f-abc0defg.bedrock-runtime.us-west-2.vpce.amazonaws.com`). |
| `region`      | The region to use when configuring API clients. This is necessary because the 'frontend' binary container cannot access environment variables from the host OS.                                                                                                                                                 |

Provisioned throughput for AWS Bedrock models can be configured using the `"awsBedrockProvisionedThroughput"` server-side configuration type. Refer to the [Model Overrides](/cody/enterprise/model-configuration#model-overrides) section for more details.

</Accordion>

## Self-hosted models

<Callout type="note">
	Configuring self-hosted models can be nuanced and complex. Please reach out
	to Sourcegraph support for assistance with the setup.
</Callout>

Please refer to the [self-hosted models](/cody/enterprise/model-configuration#self-hosted-models) documentation for detailed information about configuring self-hosted models.

Example configuration:

```json
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": null,
  "providerOverrides": [
    {
      "id": "fireworks",
      "displayName": "Fireworks",
      "serverSideConfig": {
        "type": "openaicompatible",
        "endpoints": [
          {
            "url": "https://api.fireworks.ai/inference/v1",
            "accessToken": "token"
          }
        ]
      }
    }
  ],
  "selfHostedModels": [
    {
      "provider": "fireworks",
      "model": "mixtral-8x7b-instruct@v1",
      "override": {
        "displayName": "(Fireworks) Mixtral 8x7b Instruct",
        "serverSideConfig": {
          "type": "openaicompatible",
          "apiModel": "accounts/fireworks/models/mixtral-8x7b-instruct"
        }
      }
    },
    {
      "provider": "fireworks",
      "model": "mixtral-8x22b-instruct@v1",
      "override": {
        "displayName": "(Fireworks) Mixtral 8x22b Instruct",
        "serverSideConfig": {
          "type": "openaicompatible",
          "apiModel": "accounts/fireworks/models/mixtral-8x22b-instruct"
        }
      }
    },
  ]
}
```

{/* In the configuration above: TODO */}
