# Model Configuration

<p className="subtitle">Learn how to configure Cody via `modelConfigurations` on a Sourcegraph Enterprise instance.</p>

For Sourcegraph `v5.6.0` and more, users on an Enterprise Sourcegraph instance can choose between different LLM models for Cody chat, enabling greater flexibility to select the best model for their needs.

The LLM models available for use from a Sourcegraph Enterprise instance are the union of Sourcegraph-supplied models and any custom models providers that you explicitly add to your Sourcegraph instance's site configuration.

The model configuration for Cody is managed through the `"modelConfiguration"` field in the **Site config** section.

It includes the following fields:

|                                 **Field**                                  |                                                                                        **Description**                                                                                         |
| ---------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [`sourcegraph`](/cody/model-configuration#sourcegraph-supplied-models) | Configures access to Sourcegraph-supplied models available through Cody Gateway.                                                                                                           |
| [`providerOverrides`](/cody/model-configuration#provider-overrides)    | Configures model providers, allowing you to customize Cody's connection to model providers, such as using your own API keys or self-hosted models.                                         |
| [`modelOverrides`](/cody/model-configuration#model-overrides)          | Extends or modifies the list of models Cody recognizes, along with their configurations.                                                                                                   |
| [`selfHostedModels`](/cody/model-configuration#self-hosted-models)     | Adds models to Cody’s recognized models list with default configurations provided by Sourcegraph. Only available for certain models; general models can be configured in `modelOverrides`. |
| [`defaultModels`](/cody/model-configuration#default-models)            | Specifies the models assigned to each Cody feature (chat, fast chat, autocomplete).                                                                                                        |

## Getting started with `modelConfiguration`

The recommended way to set up model configuration is by using Sourcegraph-supplied models through the Cody Gateway.

For a minimal configuration example, see [Configure Sourcegraph-supplied models](/cody/model-configuration#configure-sourcegraph-supplied-models).

## Sourcegraph-supplied models

Sourcegraph-supplied models, accessible through the [Cody Gateway](/cody/core-concepts/cody-gateway), are managed via your site configuration.

For most administrators, relying on these models alone ensures access to high-quality models without needing to manage specific configurations.

Usage of these models is controlled through the `"modelConfiguration.sourcegraph"` field in the site config.

### Disable Sourcegraph-supplied models

To disable all Sourcegraph-supplied models and use only the models explicitly defined in your site configuration, set the `"sourcegraph"` field to `null` (see example below).

```json
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": null, // ignore Sourcegraph-supplied models
  "providerOverrides": {
    // define access to the LLM providers
  },
  "modelOverrides": {
    // define models available via providers defined in the providerOverrides
  },
  "defaultModels": {
    // set default models per Cody feature from the list of models defined in modelOverrides
  }
}
```

### Configure Sourcegraph-supplied models

The minimal configuration for Sourcegraph-supplied models is:

```json
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": {}
}
```

The above configuration sets up the following:

-   Sourcegraph-supplied models are enabled (`sourcegraph` is not set to `null`).
-   Requests to LLM providers are routed through the Cody Gateway (no `providerOverrides` field specified).
-   Sourcegraph-defined default models are used for Cody features (no `defaultModels` field specified).

There are three main settings for configuring Sourcegraph-supplied LLM models:

|                           Field                           |                                              Description                                               |
| --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| `endpoint`                                                | (Optional) The URL for connecting to Cody Gateway, defaulting to the production instance.              |
| `accessToken`                                             | (Optional) The access token for connecting to Cody Gateway, which defaults to the current license key. |
| [`modelFilters`](/cody/model-configuration#model-filters) | (Optional) Filters specifying which models to include from Cody Gateway.                               |

### Model Filters

The `"modelFilters"` section allows you to control which Cody Gateway models are available to users of your Sourcegraph Enterprise instance. The following table describes each field:

|     Field      |                                                                            Description                                                                             |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `statusFilter` | Filters models by their release status, such as "stable", "beta", "deprecated" or "experimental." By default, all models available on Cody Gateway are accessible. |
| `allow`        | An array of `modelRef`s specifying which models to include. Supports wildcards.                                                                                    |
| `deny`         | An array of `modelRef`s specifying which models to exclude. Supports wildcards.                                                                                    |

The following examples demonstrate how to use each of these settings together:

```json
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": {
    "modelFilters": {
      // Only allow "beta" and "stable" models.
      // Not "experimental" or "deprecated".
      "statusFilter": ["beta", "stable"],

      // Allow any models provided by Anthropic, OpenAI, Google and Fireworks.
      "allow": [
        "anthropic::*", // Anthropic models
        "openai::*", // OpenAI models
        "google::*", // Google Gemini models
        "fireworks::*", // Autocomplete models like StarCoder and DeepSeek-V2-Coder hosted on Fireworks
      ],

      // Do not include any models with the Model ID containing "turbo",
      // or any from AcmeCo.
      "deny": [
        "*turbo*",
        "acmeco::*"
      ]
    }
  }
}
```

## Provider Overrides

A "provider" is an organizational concept for grouping LLM models. Typically, a provider refers to the company that
produced the model or the specific API/service used to access it, serving as a namespace.

By defining a provider override in your Sourcegraph site configuration, you can introduce a new namespace to organize
models or customize the existing provider namespace supplied by Sourcegraph (e.g., for all `"anthropic"` models).

Provider overrides are configured via the `"modelConfiguration.providerOverrides"` field in the site configuration.
This field is an array of items, each containing the following fields:

|       Field        |                                    Description                                    |
| ------------------ | --------------------------------------------------------------------------------- |
| `id`               | The namespace for models accessed via the provider.                               |
| `displayName`      | A human-readable name for the provider.                                           |
| `serverSideConfig` | Defines how to access the provider. See the section below for additional details. |

Example configuration:

```json
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": {},
  "providerOverrides": [
      {
        "id": "openai",
        "displayName": "OpenAI (via BYOK)",
        "serverSideConfig": {
          "type": "openai",
          "accessToken": "token",
          "endpoint": "https://api.openai.com"
        }
      },
  ],
  "defaultModels": {
    "chat": "openai::v1::google::v1::gemini-1.5-pro",
    "fastChat": "anthropic::2023-06-01::claude-3-haiku",
    "autocomplete": "fireworks::v1::deepseek-coder-v2-lite-base"
  }
}
```

In the example above:

-   Sourcegraph-supplied models are enabled (`sourcegraph` is not set to `null`).
-   The `"openai"` provider configuration is overridden to be accessed using your own key, with the provider API accessed
    directly via the specified `endpoint` URL. In contrast, models from the `"anthropic"` provider are accessed through the Cody Gateway.

For additional examples, refer to the [examples page](/cody/model-configuration/examples).

### Server-side Config

The most important part of a provider's configuration is the `"serverSideConfig"` field, which defines how the LLM models
should be invoked—that is, which external service or API will handle the LLM requests.

Sourcegraph natively supports several types of LLM API providers. The current set of supported providers includes:

|    Provider type     |                                                 Description                                                 |
| -------------------- | ----------------------------------------------------------------------------------------------------------- |
| `"sourcegraph"`      | [Cody Gateway](/cody/core-concepts/cody-gateway), offering access to various models from multiple services. |
| `"openaicompatible"` | Any OpenAI-compatible API implementation.                                                                   |
| `"awsBedrock"`       | [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/)                                                      |
| `"azureOpenAI"`      | [Microsoft Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service/)            |
| `"anthropic"`        | [Anthropic](https://www.anthropic.com)                                                                      |
| `"fireworks"`        | [Fireworks AI](https://fireworks.ai)                                                                        |
| `"google"`           | [Google Gemini](http://cloud.google.com/gemini) and [Vertex](https://cloud.google.com/vertex-ai/)           |
| `"openai"`           | [OpenAI](http://platform.openai.com)                                                                        |
| `"huggingface-tgi"`  | [Hugging Face Text Generation Interface](https://huggingface.co/docs/text-generation-inference/en/index)    |

The configuration for `serverSideConfig` varies by provider type.

For examples, refer to the [examples page](/cody/model-configuration/examples).

## Model Overrides

With a provider defined (either a Sourcegraph-supplied provider or a custom provider configured via the `providerOverrides`
field), custom models can be specified for that provider by adding them to the `"modelConfiguration.modelOverrides"` section.

This field is an array of items, each with the following fields:

-   `modelRef` - Uniquely identifies the model within the provider namespace.
    -   A string in the format `${providerId}::${apiVersionId}::${modelId}`.
    -   To associate a model with your provider, `${providerId}` must match the provider’s ID.
    -   `${modelId}` can be any URL-safe string.
    -   `${apiVersionId}` specifies the API version, which helps detect compatibility issues between models and Sourcegraph instances.
        For example, `"2023-06-01"` can indicate that the model uses that version of the Anthropic API. If unsure, you may set this to `"unknown"` when defining custom models.
-   `displayName` - An optional, user-friendly name for the model. If not set, clients should display the `ModelID` part of the `modelRef` instead (not the `modelName`).
-   `modelName` - A unique identifier used by the API provider to specify which model is being invoked.
    This is the identifier that the LLM provider recognizes to determine the model you are calling.
-   `capabilities` - A list of capabilities that the model supports. Supported values: "autocomplete" and "chat".
-   `category` - Specifies the model's category, with the following options:

    -   `"balanced"` - Typically the best default choice for most users. This category is suited for state-of-the-art models, like Sonnet 3.5 (as of October 2024).
    -   `"speed"` - Ideal for low-parameter models that may not be suited for general-purpose chat but are beneficial for specialized tasks, such as query rewriting.
    -   `"accuracy"` - Reserved for models, like OpenAI o1, that use advanced reasoning techniques to improve response accuracy, though with slower latency.
    -   `"other"` - Used for older models without distinct advantages in reasoning or speed. Select this category if uncertain about which category to choose.
    -   `"deprecated"` - For models that are no longer supported by the provider and are filtered out on the client side (not available for use).

-   `contextWindow` - An object that defines the number of "tokens" (units of text) that can be sent to the LLM.
    This setting influences response time and request cost, and may vary according to the limits set by each LLM model or provider.
    It includes two fields:
    -   `maxInputTokens` - Specifies the maximum number of tokens for the contextual data in the prompt (e.g., question, relevant snippets).
    -   `maxOutputTokens` - Specifies the maximum number of tokens allowed in the response.
-   `serverSideConfig` - Additional configuration for the model. Can be one of the following:

    -   `awsBedrockProvisionedThroughput` - Specifies provisioned throughput settings for AWS Bedrock models, with the following fields:

        -   `type` - Must be `"awsBedrockProvisionedThroughput"`.
        -   `arn` - The ARN (Amazon Resource Name) for provisioned throughput to use when sending requests to AWS Bedrock.

    -   `openaicompatible` - Configuration specific to models provided by an OpenAI-compatible provider, with the following fields:

        -   `type` - Must be `"openaicompatible"`.
        -   `apiModel` - The literal string value of the `model` field to be sent to the `/chat/completions` API.
            If set, Sourcegraph treats this as an opaque string and sends it directly to the API without inferring any additional information.
            By default, the configured model name is sent.

-   `clientSideConfig` - Advanced configuration options that are only respected if the model is provided by the `"openaicompatible"` provider.
    TODO: add more details.

Example configuration:

```json
"cody.enabled": true,
"modelConfiguration": {
  "sourcegraph": {},
  "providerOverrides": [
    {
      "id": "huggingface-codellama",
      "displayName": "huggingface",
      "serverSideConfig": {
        "type": "openaicompatible",
        "endpoints": [
          {
            "url": "https://api-inference.huggingface.co/models/meta-llama/CodeLlama-7b-hf/v1/",
            "accessToken": "token"
          }
        ]
      }
    }
  ],
  "modelOverrides": [
    {
      "modelRef": "huggingface-codellama::v1::CodeLlama-7b-hf",
      "modelName": "CodeLlama-7b-hf",
      "displayName": "(HuggingFace) CodeLlama-7b-hf",
      "contextWindow": {
        "maxInputTokens": 8192,
        "maxOutputTokens": 4096
      },
      "serverSideConfig": {
        "type": "openaicompatible",
        "apiModel": "meta-llama/CodeLlama-7b-hf"
      },
      "clientSideConfig": {
        "openaicompatible": {}
      },
      "capabilities": ["autocomplete", "chat"],
      "category": "balanced",
      "status": "stable"
    }
  ],
  "defaultModels": {
    "chat": "openai::v1::google::v1::gemini-1.5-pro",
    "fastChat": "anthropic::2023-06-01::claude-3-haiku",
    "autocomplete": "huggingface-codellama::v1::CodeLlama-7b-hf"
  }
}
```

In the example above:

-   Sourcegraph-supplied models are enabled (`sourcegraph` is not set to `null`).
-   An additional provider, `"huggingface-codellama"`, is configured to access Hugging Face’s OpenAI-compatible API directly.
-   A custom model, `"CodeLlama-7b-hf"`, is added using the `"huggingface-codellama"` provider.
-   Default models are set up as follows:
    -   Sourcegraph-supplied models are used for `"chat"` and `"fastChat"` (accessed via Cody Gateway).
    -   The newly configured model, `"huggingface-codellama::v1::CodeLlama-7b-hf"`, is used for `"autocomplete"`
        (connecting directly to Hugging Face’s OpenAI-compatible API).

For additional examples, refer to the [examples page](/cody/model-configuration/examples).

## Self-hosted Models

<Callout type="note">
	Configuring self-hosted models can be nuanced and complex. Consider reaching
	out to Sourcegraph support for assistance with the setup.
</Callout>

With a provider defined (either a Sourcegraph-supplied provider or a custom provider configured via the `providerOverrides` field),
self-hosted models can be added to Cody’s recognized models list by configuring the `"modelConfiguration.selfHostedModels"` section.
These models use Sourcegraph’s default configuration settings and are only available for select models.
Generic models should instead be configured in the `"modelOverrides"` section.

Specifying the same model in both `"selfHostedModels"` and `"modelOverrides"` is not allowed.

This field is an array of items, each with the following fields:

|    Field     |                                                                                                                                                                      Description                                                                                                                                                                      |
| ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `provider`   | (Required) The provider ID of the model, configured in the `providerOverrides` field.                                                                                                                                                                                                                                                                 |
| `apiVersion` | (Optional) The API version to use. Defaults to `"v1"` if not specified.                                                                                                                                                                                                                                                                               |
| `model`      | (Optional) Specifies which default model configuration to use. Sourcegraph provides default configurations for select models. For arbitrary models, use the `modelOverrides` section. Available options include: `"starcoder2-7b@v1"`, `"starcoder2-15b@v1"`, `"mistral-7b-instruct@v1"`, `"mixtral-8x7b-instruct@v1"`, `"mixtral-8x22b-instruct@v1"` |

For examples, refer to the [examples page](/cody/model-configuration/examples#self-hosted-models).

## Default Models

The `"modelConfiguration"` setting includes a `"defaultModels"` field, which allows you to specify the LLM model used for
each Cody feature (`"chat"`, `"fastChat"`, and `"autocomplete"`). The values for each feature should be `modelRef`s of
either Sourcegraph-supplied models or models configured in the `modelOverrides` section.

If no default is specified or if the specified model is not found, the configuration will silently fall back to a suitable alternative.

Example configuration:

```json
"cody.enabled": true,

// allow Sourcegraph-supplied models
"sourcegraph": {},

"defaultModels": {
  "chat": "openai::2024-02-01::gpt-4o",
  "fastChat": "google::v1::gemini-1.5-flash",
  "codeCompletion": "fireworks::v1::deepseek-coder-v2-lite-base"
}
```

## View Configuration

To view the current model configuration, run the following command:

```bash
export INSTANCE_URL="https://sourcegraph.test:3443"  # Replace with your Sourcegraph instance URL
export ACCESS_TOKEN="your access token"

curl --location "${INSTANCE_URL}/.api/modelconfig/supported-models.json" \
--header "Authorization: token $ACCESS_TOKEN"
```

The response includes:

-   Configured providers and models—both Sourcegraph-supplied (if enabled, with any applied filters) and any overrides.
-   Default models for Cody features.

Example response:

```json
"schemaVersion": "1.0",
"revision": "0.0.0+dev",
"providers": [
    {
        "id": "anthropic",
        "displayName": "Anthropic"
    },
    {
        "id": "fireworks",
        "displayName": "Fireworks"
    },
    {
        "id": "google",
        "displayName": "Google"
    },
    {
        "id": "openai",
        "displayName": "OpenAI"
    },
    {
        "id": "mistral",
        "displayName": "Mistral"
    }
],
"models": [
    {
        "modelRef": "anthropic::2024-10-22::claude-3-5-sonnet-latest",
        "displayName": "Claude 3.5 Sonnet (Latest)",
        "modelName": "claude-3-5-sonnet-latest",
        "capabilities": ["chat"],
        "category": "accuracy",
        "status": "stable",
        "tier": "free",
        "contextWindow": {
            "maxInputTokens": 45000,
            "maxOutputTokens": 4000
        }
    },
    {
        "modelRef": "anthropic::2023-06-01::claude-3-opus",
        "displayName": "Claude 3 Opus",
        "modelName": "claude-3-opus-20240229",
        "capabilities": ["chat"],
        "category": "other",
        "status": "stable",
        "tier": "pro",
        "contextWindow": {
            "maxInputTokens": 45000,
            "maxOutputTokens": 4000
        }
    },
    {
        "modelRef": "anthropic::2023-06-01::claude-3-haiku",
        "displayName": "Claude 3 Haiku",
        "modelName": "claude-3-haiku-20240307",
        "capabilities": ["chat"],
        "category": "speed",
        "status": "stable",
        "tier": "free",
        "contextWindow": {
            "maxInputTokens": 7000,
            "maxOutputTokens": 4000
        }
    },
    {
        "modelRef": "fireworks::v1::starcoder",
        "displayName": "StarCoder",
        "modelName": "starcoder",
        "capabilities": ["autocomplete"],
        "category": "speed",
        "status": "stable",
        "tier": "free",
        "contextWindow": {
            "maxInputTokens": 2048,
            "maxOutputTokens": 256
        }
    },
    {
        "modelRef": "fireworks::v1::deepseek-coder-v2-lite-base",
        "displayName": "DeepSeek V2 Lite Base",
        "modelName": "accounts/sourcegraph/models/deepseek-coder-v2-lite-base",
        "capabilities": ["autocomplete"],
        "category": "speed",
        "status": "stable",
        "tier": "free",
        "contextWindow": {
            "maxInputTokens": 2048,
            "maxOutputTokens": 256
        }
    },
    {
        "modelRef": "google::v1::gemini-1.5-pro",
        "displayName": "Gemini 1.5 Pro",
        "modelName": "gemini-1.5-pro",
        "capabilities": ["chat"],
        "category": "balanced",
        "status": "stable",
        "tier": "free",
        "contextWindow": {
            "maxInputTokens": 45000,
            "maxOutputTokens": 4000
        }
    },
    {
        "modelRef": "google::v1::gemini-1.5-flash",
        "displayName": "Gemini 1.5 Flash",
        "modelName": "gemini-1.5-flash",
        "capabilities": ["chat"],
        "category": "speed",
        "status": "stable",
        "tier": "free",
        "contextWindow": {
            "maxInputTokens": 45000,
            "maxOutputTokens": 4000
        }
    },
    {
        "modelRef": "mistral::v1::mixtral-8x7b-instruct",
        "displayName": "Mixtral 8x7B",
        "modelName": "accounts/fireworks/models/mixtral-8x7b-instruct",
        "capabilities": ["chat"],
        "category": "speed",
        "status": "stable",
        "tier": "free",
        "contextWindow": {
            "maxInputTokens": 7000,
            "maxOutputTokens": 4000
        }
    },
    {
        "modelRef": "openai::2024-02-01::gpt-4o",
        "displayName": "GPT-4o",
        "modelName": "gpt-4o",
        "capabilities": ["chat"],
        "category": "accuracy",
        "status": "stable",
        "tier": "pro",
        "contextWindow": {
            "maxInputTokens": 45000,
            "maxOutputTokens": 4000
        }
    },
    {
        "modelRef": "openai::2024-02-01::cody-chat-preview-001",
        "displayName": "OpenAI o1-preview",
        "modelName": "cody-chat-preview-001",
        "capabilities": ["chat"],
        "category": "accuracy",
        "status": "waitlist",
        "tier": "pro",
        "contextWindow": {
            "maxInputTokens": 45000,
            "maxOutputTokens": 4000
        }
    },
    {
        "modelRef": "openai::2024-02-01::cody-chat-preview-002",
        "displayName": "OpenAI o1-mini",
        "modelName": "cody-chat-preview-002",
        "capabilities": ["chat"],
        "category": "accuracy",
        "status": "waitlist",
        "tier": "pro",
        "contextWindow": {
            "maxInputTokens": 45000,
            "maxOutputTokens": 4000
        }
    }
],
"defaultModels": {
    "chat": "anthropic::2024-10-22::claude-3-5-sonnet-latest",
    "fastChat": "anthropic::2023-06-01::claude-3-haiku",
    "codeCompletion": "fireworks::v1::deepseek-coder-v2-lite-base"
}
```
